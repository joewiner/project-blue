Methods
Participants
Twenty-six healthy participants (10 female) with normal or corrected-to-normal vision were recruited from the general population by means of announcements: age: mean = 37.3, sd = 9.1; education level: mean = 5.9[1]; verbal IQ: mean = 24.2, sd = 5.0[2].  The inclusion criteria were age between 18 and 60 years, and fluency in Spanish.  The exclusion criteria were neurological or mental illness, intellectual disability, head injury, alcohol or drug abuse in the past six months, and current severe physical disease, as
well as the standard exclusion criteria for participation in fMRI procedures, namely claustrophobia and metallic implants including fitted pacemaker and cochlear implants.  This study was approved by the Parc Sanitari Sant Joan de Déu ethics committee and all participants provided written informed consent before taking part in the study. 
 
Scales for visual imagery, hallucinations and delusion proneness
Proneness to hallucinations was assessed by means of a Spanish adaptation of the Launay-Slade Hallucination Scale (LSHS; [20]), a self-questionnaire which assesses non-clinical hallucinations within various modalities.  Two additional items were mixed with the LSHS items, although not taken into account in the computation of the global LSHS score: ‘I can easily identify animals or things in the clouds’, and ‘When I see spots (of painting, humidity…), I can see faces, silhouettes or objects in them’.  These two items were first piloted in the general population to ensure that they led to a diversity of scores.  Similar to the LSHS items, each of these new items had to be rated from 0 to 3 by the participants according to the frequency of the experience[3].  The total score obtained on these two items constituted a visual imagery score (m = 1.77, sd = .95; range: [0,3]) and was used to define two subgroups: nine participants who obtained a visual imagery score of 0 or 1 were included in the low visual imagery subgroup (mean score: m = .67), although one of them had to be subsequently ruled out of the encoding (but not of the recall) analyses due to unreliable fMRI data; and seven participants who scored at least 2 on either of the two questions were included in the high visual imagery subgroup (mean score: m = 2.86).  The two subgroups were highly significantly different with regard to the visual imagery score (t(14) = 9.62, p <.0001).  The remaining ten participants, who obtained a visual imagery score of 2 by scoring 1 on each of the two questions, were excluded from the analyses involving visual imagery score because it was unclear whether they should be included in the high or the low visual imagery subgroup.
A global hallucination proneness score was tallied by adding up the sub-scores for all LSHS items (excluding the two new items) (m = 4.5, sd = 3.2; range: [0,11]).  Subgroups of participants with high (score ≥ 6, n = 8 for encoding and n = 7 for recall) and low (score ≤ 2, n = 8 for encoding and n = 7 for recall) global hallucination proneness score were defined.  Ten participants with a score of 3, 4, or 5 were excluded from the analyses contrasting these subgroups.  Auditory non-verbal and verbal hallucination proneness scores were computed as well by adding up the sub-scores obtained on the corresponding items.  Proneness to delusions was assessed by means of the Peters Delusion Inventory scale [21] (m = 4.7, sd = 4.5; range: [0,21]).  Subgroups of participants with high (score >= 6, n = 11 for encoding and n = 10 for recall) and low (score <=2, n = 9 for encoding and for recall) delusion proneness score were defined.  Five participants who scored 3, 4, or 5 were excluded from these subgroup comparisons.  Certain fMRI data had to be discarded due to technical problems (see below).
 
 
Material
Ninety items were selected, including 72 common objects (saw, apron, envelope...) and 18 vegetables (carrot, cauliflower, onion...).  Participants were presented with either the mere verbal label of the item or the picture of the item along with its verbal label.  Half of the items were presented as single words and the other half as word/picture pairs.  Two versions of the stimuli were prepared; the 45 stimuli that were presented as words in one version were presented as word/picture pairs in the other.  The use of each version was counterbalanced among subjects.  All material was presented to participants throughout via Presentation (http://www.neurobs.com/).  Before both the encoding and the recall phases, participants were administered a few practice trials to ensure familiarity with the task.
 
Procedure
Encoding
All items were presented one by one in pseudo-random order with each slide (word or word/picture pair) being presented for 3.5 seconds, separated by fixation crosses.  The stimulus onset asynchrony varied across trials from 5.5 to 9 seconds according to an exponential distribution with a mean of 6.68 seconds.  The encoding of the items was implicit since the task was described as a classification task: participants were required to indicate whether the presented items were vegetables or not, and to give their response by pressing one of two buttons (‘vegetable’ or ‘other’).  They were not informed of the subsequent recall task.  The computer recorded correct and incorrect responses, as well as misses (i.e., no response after 2 seconds from the end of the stimulus).
 
Recall
A 6-minute delay followed, during which a structural MRI scan was acquired for each participant.  Then the participants were informed that they would be presented with all the labels of the stimuli they had previously seen, and that they would have to remember whether a picture accompanied the item at the encoding phase.  The labels were presented one at a time for a 3.5-second period, in pseudo-random order different from that used in the encoding stage, but with similar stimulus onset asynchrony distribution.  After the appearance of each label the participants were asked to provide their response by pressing one of two buttons (‘with picture’ or ‘without picture’).  The correct responses, i.e., presented words remembered as words (WW) and presented pictures remembered as pictures (PP), were recorded, as were the incorrect responses: the omissions, i.e. presented pictures remembered as words (PW) and the false memories of pictures, i.e. presented words remembered as pictures (WP).  Again, missed trials were recorded.  
 
The response times for each type of response were recorded as well.  Prior to the experimental protocol participants were provided with an opportunity to ask any questions they had.
 
 
 
fMRI data acquisition
Functional MRI data were acquired with a 1.5T General Electric Signa HDe scanner at Parc Sanitari Sant Joan de Déu.  A T2*-weighted functional echoplanar imaging sequence depicting BOLD contrast was obtained using a quadrature head coil. In total 270 volumes were collected with slices parallel to the AC-PC plane, resulting in an axial-oblique orientation.  The following scanning parameters were used: 26 slices, 4 mm thickness, 1 mm gap, TR = 2000 ms, TE = 40 ms, 24 cm FOV, 64 × 64 acquisition matrix, flip angle = 90°.  The first seven volumes in each run were discarded to allow for magnetic saturation effects.  Visual stimuli were presented on a rear projection screen and viewed through a mirror mounted on the head coil, and all responses were collected with an MR-compatible response box (fORP, Current Designs, Inc., USA; www.curdes.com). 
 
fMRI data preprocessing
Imaging data were analyzed using SPM8 (Wellcome Department of Imaging Neuroscience, London; www.fil.ion.ucl.ac.uk/spm) running under MATLAB (Release 2009a, The MathWorks, Inc., Natick, Massachusetts).  All of the volumes from each participant were spatially realigned to the first image in each series, in order to correct for small head movements and to generate a mean image for the functional series. Motion parameters were examined for each subject to ensure no movements larger than the voxel size were present (no runs were discarded).  The resulting series were transformed into SRI24 space [22], with a standard EPI template as deformation target, and then spatially smoothed using a Gaussian kernel of 8 mm full-width-at-half-maximum.
Due to machine errors, data from the encoding task for one participant (from the low imagery subgroup) were disregarded, as were data from the recall task for two other participants (both among the ten participants excluded from the high- vs. low-imagery subgroups).
 
fMRI data analysis
            Preprocessed fMRI data were then analyzed with an event-related model, using SPM8.  In order to assess random effects at the individual level, the activity associated with the experimental conditions was modeled with a hemodynamic response function (HRF).  To control for slice timing discrepancies we also included the time derivative of the HRF in the model.  Displacement and rotation motion parameters were included as confounds in the individual model.  A 200 s high-pass filter cut-off was used to remove low frequency noise, together with an AR(1) model to correct for temporal autocorrelation.  
For both tasks, four event types were determined by the responses in the recall task: WW, PP, PW, and WP.  To compare each event, linear contrasts were constructed to test experimental effects of interest.  These contrasts were entered into a second level analysis in which subjects were treated as a random effect.  One-sample and two- sample t-tests were used to assess within-group and between-subgroup activations, respectively. 
The resulting statistical parametric maps were generated with SPM8, initially using an uncorrected voxel-level threshold defined by T = 3.0 (p < 0.003) and a cluster extent threshold defined by a family-wise-error (FWE) corrected p < 0.05.  In the event that the resulting activation clusters encompassed several brain structures, a more restrictive voxel-wise threshold, T = 3.8–4.0 (p < 0.0004), was applied to better identify local activation maxima in each brain structure, again followed by a p < 0.05 FWE-corrected threshold on the remaining clusters.  Activation peaks were labeled according to the SRI24/TZO cortical parcellation map [22] based on the template described by Tzourio-Mazoyer et al. [23].
